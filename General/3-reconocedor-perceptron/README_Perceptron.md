# Reconocedor de NÃºmeros con PerceptrÃ³n Multicapa

## DescripciÃ³n

Este programa implementa un reconocedor de dÃ­gitos utilizando un **perceptrÃ³n multicapa** (red neuronal artificial) en lugar del sistema de comparaciÃ³n de patrones original. El perceptrÃ³n aprende a reconocer dÃ­gitos del 0 al 9 mediante entrenamiento supervisado.

## Arquitectura del PerceptrÃ³n

### Estructura de la Red
- **Capa de entrada**: 100 neuronas (10Ã—10 pÃ­xeles normalizados)
- **Capa oculta**: 50 neuronas
- **Capa de salida**: 10 neuronas (una por cada dÃ­gito 0-9)

### Funciones de ActivaciÃ³n
- **FunciÃ³n sigmoid**: `f(x) = 1 / (1 + e^(-x))`
- **Derivada**: `f'(x) = f(x) * (1 - f(x))`

## Algoritmo de Entrenamiento

### Backpropagation
1. **PropagaciÃ³n hacia adelante (Forward Pass)**:
   - Calcula la salida de cada capa usando los pesos actuales
   - Aplica la funciÃ³n sigmoid a las activaciones

2. **PropagaciÃ³n hacia atrÃ¡s (Backward Pass)**:
   - Calcula el error en la capa de salida
   - Propaga el error hacia las capas anteriores
   - Actualiza pesos y bias usando descenso de gradiente

### ParÃ¡metros de Entrenamiento
- **Ã‰pocas**: 50 (configurable)
- **Tasa de aprendizaje**: 0.1
- **InicializaciÃ³n de pesos**: Xavier (para mejor convergencia)

## Flujo de Trabajo

### 1. PreparaciÃ³n de Datos
- Carga imÃ¡genes BMP del dataset
- Convierte a escala de grises
- Binariza con umbral 128
- Segmenta regiones conectadas
- Normaliza cada regiÃ³n a 10Ã—10 pÃ­xeles

### 2. Entrenamiento
- Convierte patrones 10Ã—10 en vectores de 100 elementos
- Crea vectores one-hot para las etiquetas (0-9)
- Entrena el perceptrÃ³n con backpropagation
- Muestra progreso y estadÃ­sticas

### 3. Reconocimiento
- Procesa nueva imagen de la misma manera
- Alimenta el patrÃ³n normalizado al perceptrÃ³n
- Obtiene probabilidades para cada dÃ­gito
- Selecciona el dÃ­gito con mayor probabilidad

## CaracterÃ­sticas del Sistema

### Interfaz Mejorada
- **BotÃ³n "Reentrenar PerceptrÃ³n"**: Permite reentrenar la red
- **Indicador de estado**: Muestra si el perceptrÃ³n estÃ¡ entrenado
- **VisualizaciÃ³n de confianza**: Colores basados en la confianza de predicciÃ³n
  - Verde: >80% confianza
  - Naranja: 60-80% confianza
  - Rojo: <60% confianza

### Ventajas sobre el Sistema Anterior
1. **Aprendizaje adaptativo**: Mejora con mÃ¡s datos
2. **GeneralizaciÃ³n**: Puede reconocer variaciones no vistas en entrenamiento
3. **Probabilidades**: Proporciona medidas de confianza mÃ¡s sofisticadas
4. **Escalabilidad**: FÃ¡cil de extender para mÃ¡s clases

### Limitaciones
1. **Dependencia de datos**: Requiere dataset de entrenamiento
2. **Tiempo de entrenamiento**: MÃ¡s lento que comparaciÃ³n directa
3. **Overfitting**: Puede memorizar en lugar de generalizar
4. **ParÃ¡metros**: Requiere ajuste de hiperparÃ¡metros

## Uso del Programa

1. **Inicio**: El perceptrÃ³n se entrena automÃ¡ticamente al abrir
2. **Cargar imagen**: Selecciona una imagen BMP con dÃ­gitos
3. **Procesar**: El perceptrÃ³n reconoce los dÃ­gitos encontrados
4. **Reentrenar**: Si es necesario, puede reentrenar la red

## Archivos Requeridos

- `reconocedor_numeros.py`: Programa principal
- `Numeros/`: Carpeta con subcarpetas 0-9 conteniendo imÃ¡genes BMP de entrenamiento

## Dependencias

- `tkinter`: Interfaz grÃ¡fica
- `numpy`: Operaciones matemÃ¡ticas
- `collections.deque`: Para BFS en segmentaciÃ³n
- `os`: Manejo de archivos
- `random`: InicializaciÃ³n de pesos

## Notas TÃ©cnicas

### InicializaciÃ³n de Pesos
Se usa la inicializaciÃ³n de Xavier para evitar el problema de desvanecimiento/explosiÃ³n de gradientes:
```
limite = sqrt(2.0 / numero_neuronas_capa_anterior)
peso = random.uniform(-limite, limite)
```

### NormalizaciÃ³n de Datos
Los patrones se normalizan a 10Ã—10 pÃ­xeles usando interpolaciÃ³n bilineal para mantener la proporciÃ³n y calidad de la imagen.

### FunciÃ³n de PÃ©rdida
Se usa el error cuadrÃ¡tico medio (MSE) para medir la diferencia entre predicciones y valores reales durante el entrenamiento.

## âš¡ï¸ Â¿DÃ³nde estÃ¡ implementado el perceptrÃ³n en el cÃ³digo?

---

### ðŸ“‚ Archivo: `reconocedor_numeros_perc.py`

#### ðŸ§  **Toda la lÃ³gica del perceptrÃ³n estÃ¡ implementada en la clase `Perceptron` (lÃ­neas 7 a 74 aprox.)**

```python
class Perceptron:
    """ImplementaciÃ³n de un perceptrÃ³n multicapa para reconocimiento de dÃ­gitos"""
    def __init__(self, capas):
        # Inicializa la arquitectura y llama a inicializar_pesos()
    def inicializar_pesos(self):
        # Inicializa pesos y bias aleatoriamente (Xavier)
    def sigmoid(self, x):
        # FunciÃ³n de activaciÃ³n sigmoid
    def sigmoid_derivada(self, x):
        # Derivada de la sigmoid
    def forward(self, entrada):
        # PropagaciÃ³n hacia adelante
    def backward(self, entrada, salida_deseada, tasa_aprendizaje=0.1):
        # Backpropagation (ajuste de pesos)
    def entrenar(self, datos_entrenamiento, epocas=100, tasa_aprendizaje=0.1):
        # Entrenamiento completo (llama forward y backward)
    def predecir(self, entrada):
        # PredicciÃ³n de salida
```

- **La clase `Perceptron` es completamente independiente y NO usa librerÃ­as externas de machine learning.**
- **Todo el aprendizaje, ajuste de pesos y predicciÃ³n se hace desde cero usando solo numpy.**

#### ðŸ“Œ Â¿DÃ³nde se usa el perceptrÃ³n en el flujo del programa?

- **InicializaciÃ³n:**
  ```python
  self.perceptron = Perceptron([100, 50, 10])
  ```
  (En el constructor de `ReconocedorNumeros`)

- **Entrenamiento:**
  ```python
  self.perceptron.entrenar(datos_entrenamiento, epocas=50, tasa_aprendizaje=0.1)
  ```
  (En el mÃ©todo `entrenar_con_dataset`)

- **PredicciÃ³n:**
  ```python
  salida = self.perceptron.predecir(entrada)
  digito_predicho = np.argmax(salida)
  ```
  (En el mÃ©todo `reconocer_digito`)

---

## ðŸ§© **Resumen de la arquitectura y funciones**

- **`__init__`**: Define la arquitectura (capas) y llama a la inicializaciÃ³n de pesos.
- **`inicializar_pesos`**: Inicializa pesos y bias aleatoriamente (Xavier).
- **`sigmoid` y `sigmoid_derivada`**: FunciÃ³n de activaciÃ³n y su derivada.
- **`forward`**: PropagaciÃ³n hacia adelante (cÃ¡lculo de activaciones).
- **`backward`**: Backpropagation (ajuste de pesos y bias).
- **`entrenar`**: Entrenamiento completo (llama forward y backward para cada ejemplo).
- **`predecir`**: PropagaciÃ³n hacia adelante para obtener la predicciÃ³n.

---

## ðŸ·ï¸ **Â¿DÃ³nde modificar si quieres cambiar el perceptrÃ³n?**
- Cambia la arquitectura (nÃºmero de capas/neuronas) en la lÃ­nea de inicializaciÃ³n: `Perceptron([100, 50, 10])`
- Cambia la funciÃ³n de activaciÃ³n o el algoritmo de entrenamiento dentro de la clase `Perceptron`.

---

## ðŸ”Ž **Referencia rÃ¡pida**
- **Clase principal:** `Perceptron` (lÃ­neas 7-74 aprox.)
- **Uso en el flujo:** MÃ©todos de la clase `ReconocedorNumeros` (`entrenar_con_dataset`, `reconocer_digito`)
- **No depende de librerÃ­as externas de ML**

---
